{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **CS224W - Colab 3**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scpd-proed/XCS224W-Colab3/blob/main/Notebook/XCS224W_Colab3.ipynb)\n",
    "\n",
    "Before opening the colab with the badge, you would need to allow Google Colab to access the GitHub private repositories. Please check therefore [this tutorial](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#:~:text=Navigate%20to%20http%3A%2F%2Fcolab,to%20read%20the%20private%20files.).\n",
    "\n",
    "If colab is opened with this badge, make sure please **save copy to drive** in 'File' menu before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gzsP50bF6Gb"
   },
   "source": [
    "In Colab 2 you constructed GNN models by using PyTorch Geometric's built in GCN layer, `GCNConv`. In this Colab you will go a step deeper and implement your GNN layers directly: **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) and **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)). Using these GNN layers you will run and test your models on the CORA dataset, a standard citation network benchmark dataset.\n",
    "\n",
    "Next, you will learn how to use [DeepSNAP](https://snap.stanford.edu/deepsnap/), a Python library enabling efficient deep learning on graphs. With DeepSNAP you will learn how to easily split graphs in different ways and apply graph dataset transformations.\n",
    "\n",
    "Lastly, using DeepSNAP's transductive link prediction dataset spliting functionality, you will construct a simple GNN model for the task of edge property prediction (link prediction).\n",
    "\n",
    "**Note**: Make sure to **sequentially run all the cells in each section** so that the intermediate variables / packages will carry over to the next cell\n",
    "\n",
    "Have fun and good luck on Colab 3 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcqscyiXMuRX"
   },
   "source": [
    "## Building + Debugging Notes\n",
    "While working through this Colab and future Colabs, we strongly encourage you to follow a couple of building / debugging strategies:\n",
    "- During debugging make sure to run your notebook using the CPU runtime. You can change the notebook runtime by selecting `Runtime` and then `Change runtime type`. From the dropdown, select `None` as the `hardware accelerator`.\n",
    "- When working with PyTorch and Neural Network models, understanding the shapes of different tensors, especially the input and output tensors is incredibly helpful.\n",
    "- When training models, it is helpful to start by only running 1 epoch or even just a couple of batch iterations. This way you can check that all your tensor shapes and logic match up, while also tracking expected behavior, such as a decreasing training loss. Remember to comment out / save the default number of epochs that we provide you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSaetj53YnT6"
   },
   "source": [
    "# Device\n",
    "We recommend using a GPU for this Colab.\n",
    "\n",
    "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67gOQITlCNQi"
   },
   "source": [
    "## Setup\n",
    "First let us check which version of PyTorch you are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Install PyTorch\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    !pip install torch==2.5.1+cu124 -f https://download.pytorch.org/whl/torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vkP8pA1qBE5",
    "outputId": "25fb8a3b-90ac-465b-c6b9-f152d1f4e9d6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TIkPY1Abwq-"
   },
   "source": [
    "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J_m9l6OYCQZP",
    "outputId": "da0f1408-4423-4005-9e3b-c39841a4816d"
   },
   "outputs": [],
   "source": [
    "# Install torch geometric\n",
    "import os\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
    "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
    "  !pip install torch-geometric\n",
    "  # Fix for Deepsnap PyG 2.4.x compatibility issue (https://github.com/snap-stanford/deepsnap/issues/53)\n",
    "  !pip install -q git+https://github.com/SebastianHurubaru/deepsnap.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PRfgbfTjCRD_",
    "outputId": "0077f435-1e92-4afa-8732-fabd199c222a"
   },
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoXlf4MtYrbz"
   },
   "source": [
    "# 1) GNN Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQy2RBfgYut4"
   },
   "source": [
    "## Implementing Layer Modules\n",
    "\n",
    "In Colab 2, you implemented a GCN model for node and graph classification tasks. However, for that notebook you took advantage of PyG's built in GCN module, similar to the way you might use PyTorch's built in CNN layer. For Colab 3, we provide a general Graph Neural Network Stack (i.e. the logic for applying multiple GNN layers with a post-message passing classification head). Using this flexible class definition, you will plugin and compare the performance of your own message passing layer implementations: GraphSAGE and GAT.\n",
    "\n",
    "You will use your layer implemenations for node classification on the CORA dataset, a standard citation network benchmark. In this dataset, nodes correspond to documents and edges correspond to undirected citations. Each node or document in the graph is assigned a class label and features based on the documents binarized bag-of-words representation. Specifically, the Cora graph has 2708 nodes, 5429 edges, 7 prediction classes, and 1433 features per node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ne6Gw-CT5G"
   },
   "source": [
    "## GNN Stack Module\n",
    "\n",
    "Below is our provided implementation of a general GNN stack, where you can plugin any GNN layer, such as **GraphSage**, **GAT**, etc. This module is provided for you. Your implementations of the **GraphSage** and **GAT** layers will function as components in the GNNStack Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ys8vZAFPCWWe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim, args))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim, args))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(args.dropout), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage\n",
    "        elif model_type == 'GAT':\n",
    "            # When applying GAT with num heads > 1, you need to modify the \n",
    "            # input and output dimension of the conv layers (self.convs),\n",
    "            # to ensure that the input dim of the next layer is num heads\n",
    "            # multiplied by the output dim of the previous layer.\n",
    "            # HINT: In case you want to play with multiheads, you need to change the for-loop that builds up self.convs to be\n",
    "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
    "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
    "            return GAT\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "          \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nW_XpEwASNZ"
   },
   "source": [
    "## Creating Your Own Message Passing Layer\n",
    "\n",
    "Now it is time to implement your own message passing layers! Working through this part will help you become acutely familiar with the behind the scenes work of implementing Pytorch Message Passing Layers, allowing you to build you own custom GNN models. In doing so, you will work with and implement 3 critcal functions needed to define a PyG Message Passing Layer: `forward`, `message`, and `aggregate`.\n",
    "\n",
    "Before diving head first into the coding details, let us quickly review the key components of the message passing process. First, we focus on a single round of messsage passing with respect to a single node $x$, which we refer to as the central node. The goal of the $lth$ layer of message passing is to update $x$'s feature vector from $x^{l-1}$ to $x^l$. To do so, we implement the following steps: 1) each neighboring node $v$ passes its current message $v^{l-1}$ across the edge $(v, x)$ - 2) for the node $x$, we aggregate all the messages of neighboring nodes (for example through a sum or mean) - and 3) we transform the aggregated information by e.g. applying linear and non-linear transformations. Altogether, the message passing process is applied such that every node $u$ in our graph updates its embedding through acting as the central node $x$ in step 1-3 described above. \n",
    "\n",
    "Now, let's apply this process of propagating and transforming information within the graph to that of coding a single message passing layer. Overall, the general paradigm of message passing layers is: 1) pre-processing -> 2) **message passing** / propagation -> 3) post-processing. The `forward` function that you will implement captures this execution logic. Namely, the `forward` function handles the pre and post-processing of node features / embeddings, as well as initiates message passing by calling the `propagate` function. \n",
    "\n",
    "\n",
    "The `propagate` function encapsulates the actual message passing process within the graph! It does so by calling three important functions: 1) `message`, 2) `aggregate`, and 3) `update`. Your implementation will vary slightly from this, as you will not explicitly implement `update`, but instead place the logic for updating node embeddings after message passing and within the `forward` function. To be more specific, after information is propagated (message passing + aggregation), we can further transform the node embeddings outputed by `propagate`. Overall, the output of `forward` is exactly the node embeddings after one GNN layer.\n",
    "\n",
    "Lastly, before starting to implement our own layer, let us dig a bit deeper into each of the functions described above:\n",
    "\n",
    "1. \n",
    "\n",
    "```\n",
    "def propagate(edge_index, x=(x_src, x_dst), extra=(extra_src, extra_dst), size=size):\n",
    "```\n",
    "Calling `propagate` initiates the message passing process. Looking at the function parameters, we highlight a couple of key parameters. \n",
    "\n",
    "  - `edge_index` is passed to the forward function and captures the edge structure of the graph. `edge_index` is of shape - `[2, E]`, where `edge_index[:, e]` represents a single edge $e$ in the graph.\n",
    "  - `x=(x_src, x_dst)` represents the node features that will be used in message passing, where we have the flexibility to distinguish the features of source (src) vs. destination (dst) nodes (e.g. with different transformations). Common notation is to think of the src node features as the node features used for sending messages and dst node features as the central node features that will be receiving these messages. Note, while in this notebook the src and dst matrices will be the same shape - $[N, d]$ (i.e. coming from the same set of nodes), we will see later in the course cases where these nodes can differ.\n",
    "\n",
    "  - `extra=(extra_src, extra_dst)` represents additional information that we can associate with each node beyond its current feature embedding. In fact, we can include as many additional parameters of the form `param=(param_src, param_dst)` as we would like. We highlight that differentiating betwee `_src` and `_dst` allows us to differentiate the features used for central and neighboring node features later in the message passing process. \n",
    "\n",
    "  The output of the `propagate` function is a matrix of node embeddings after the message passing process and has shape $[N, d]$.\n",
    "\n",
    "2. \n",
    "```\n",
    "def message(x_j, ...):\n",
    "```\n",
    "The `message` function is called by `propagate` and constructs the messages from neighboring nodes (src nodes) to central nodes (dst nodes). Following PyG convention, we subscript variables dealing with src nodes with `_j` and dst nodes  with `_i`. \n",
    "\n",
    "  - `x_j` represents the matrix of node features for *each src node of each edge* in the graph. Specifically, messages are constructed along each edge $(j, i) \\in E$, where edge_index is used extract node feature information to construct `x_j` as `x_j = x_src[edge_index[0, :], :]`. Thus, `x_j` has shape $[|E|, d]$!\n",
    "\n",
    "  - `...` represent any additional arguments passed to `propagate`, from which we can differentiate between src and dst node types by appending `_i` or `_j` to the variable name. In implementing GAT you will see how you can leverage accessing additional variables passed to propagate.\n",
    "\n",
    "  Critically, we emphasize that the output of the `message` function is a matrix of messages ready to be aggregated, having shape $[|E|, d]$, where these messages are constructed by applying different potential transformations to the initial src node embedding messages.\n",
    "\n",
    "  **Note** Understanding the message passing process is definitely a confusing concept; however, it is very helpful to think of the `message` function acting on each individual edges in the graph (i.e. producing a message for each directed edge $(j, i)$). Moreover, for undirected graphs since we store both directions of each edge, each node $u$ acts both as a src and dst node, sending messages to its neighbors and then also aggregating messages from its same set of neighbors.\n",
    "\n",
    "\n",
    "3. \n",
    "```\n",
    "def aggregate(self, inputs, index, dim_size = None):\n",
    "```\n",
    "Lastly, the `aggregate` function is used to aggregate the messages from neighboring nodes. Looking at the parameters, we highlight:\n",
    "\n",
    "  - `inputs` represents a matrix of the messages passed from neighboring nodes (i.e. the output of the `message` function).\n",
    "  - `index` is an array with length equal to the number of rows of `inputs` and tells us the central node $i$ associated with each message (row) in the `inputs` matrix. Thus, `index` tells us which rows / messages to aggregate for each central node $i$ - i.e. all messages with associated edges $(*, i) \\in E$.\n",
    "\n",
    "  The output of `aggregate` is of shape $[N, d]$.\n",
    "\n",
    "\n",
    "For additional resources refer to the PyG documentation for implementing custom message passing layers: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    from torch_geometric.utils import add_self_loops, degree\n",
    "    edge_index = torch.tensor([[0, 1],\n",
    "                               [1, 0],\n",
    "                               [1, 2],\n",
    "                               [2, 1],\n",
    "                               [1, 1]], dtype=torch.long)\n",
    "    edge_index=edge_index.t().contiguous()\n",
    "    row,col = edge_index\n",
    "    print(row)\n",
    "    print(col)\n",
    "    x = torch.tensor([[-1], [0], [21]], dtype=torch.float)\n",
    "    deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "    print(deg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "syDtxjxoCZgq"
   },
   "source": [
    "## GraphSage Implementation\n",
    "\n",
    "For our first GNN layer, you will implement the well known GraphSage ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) layer! \n",
    "\n",
    "For a given *central* node $v$ with current embedding $h_v^{l-1}$, the message passing update rule to tranform $h_v^{l-1} \\rightarrow h_v^l$ is as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "h_v^{(l)} = W_{dst}\\cdot h_v^{(l-1)} + W_{src} \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
    "\\end{equation}\n",
    "\n",
    "where $W_{src}$ and $W_{dst}$ are learnable weight matrices and the nodes $u$ are *neighboring* nodes. Additionally, you will use mean aggregation:\n",
    "\n",
    "\\begin{equation}\n",
    "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
    "\\end{equation}\n",
    "\n",
    "One thing to note is that we have added a **skip connection** to your GraphSage implementation through the term $W_{dst}\\cdot h_v^{(l-1)}$. \n",
    "\n",
    "Before implementing this update rule, we encourage you to think about how different parts of the GraphSage formulas correspond with the functions outlined earlier: 1) `forward`, 2) `message`, and 3) `aggregate`. As a hint, you are given what the aggregation function is (i.e. mean aggregation)! Now the question remains, what are the messages passed by each neighbor nodes and when do we call the `propagate` function? \n",
    "\n",
    "Note: in this case the message function or messages are actually quite simple. Additionally, remember that the `propagate` function encapsulates the operations of / the outputs of the combined `message` and `aggregate` functions.\n",
    "\n",
    "\n",
    "Lastly, $\\ell$-2 normalization of the node embeddings is applied after each iteration.\n",
    "\n",
    "\n",
    "<font color='red'>For the following questions, DON'T refer to any existing implementations online.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwG4HqCFCaOD"
   },
   "outputs": [],
   "source": [
    "class GraphSage(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, args, **kwargs):  \n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = args.normalize\n",
    "        bias = args.bias\n",
    "        self.lin_src = None\n",
    "        self.lin_dst = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the layers needed for the message and aggregate functions below.\n",
    "        # self.lin_src is the linear transformation that you apply to aggregated \n",
    "        #            message from neighbors.\n",
    "        # self.lin_dst is the linear transformation that you apply to embedding \n",
    "        #            for central node.\n",
    "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
    "        self.lin_src = nn.Linear(self.in_channels, args.hidden_dim)\n",
    "        self.lin_dst = nn.Linear(self.in_channels, args.hidden_dim)\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_src.reset_parameters()\n",
    "        self.lin_dst.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement message passing, as well as any post-processing (our update rule).\n",
    "        # 1. Call the propagate function to conduct message passing.\n",
    "        #    1.1 See the description of propagate above or the following link for more information: \n",
    "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        #    1.2 You will only use the representation for neighbor nodes (x_j) in message passing. \n",
    "        #        Thus, you can simply pass the same representation for src / dst as x=(x, x). \n",
    "        #        Although we give this to you, try thinking through what this means following\n",
    "        #        the descriptions above.\n",
    "        # 2. Update your node embeddings with a skip connection.\n",
    "        # 3. If normalize is set, do L-2 normalization (defined in \n",
    "        #    torch.nn.functional)\n",
    "        #\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "        prop = self.propagate(edge_index, x=(x,x))\n",
    "        prop_lin = self.lin_src(prop)\n",
    "        skip = self.lin_dst(x)\n",
    "        out = skip + prop_lin\n",
    "        if self.normalize:\n",
    "            out = torch.nn.functional.normalize(out) # might need to shape this further\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your message function here.\n",
    "        # Hint: Look at the formulation of the mean aggregation function, focusing on \n",
    "        # what message each individual neighboring node passes during aggregation.\n",
    "        #\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "        out = x_j\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        out = None\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = self.node_dim\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter: \n",
    "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
    "        #\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "        out = torch_scatter.scatter(inputs, index, dim=node_dim, dim_size=dim_size, reduce=\"mean\")\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjcfF3RACdLD"
   },
   "source": [
    "## GAT Implementation\n",
    "\n",
    "Attention mechanisms have become the state-of-the-art in many sequence-based tasks such as machine translation and learning sentence representations. One of the major benefits of attention-based mechanisms is their ability to focus on the most relevant parts of the input to make decisions. In this problem, you will learn how attention mechanisms can be used to perform node classification over graph-structured data through the usage of Graph Attention Networks (GATs) ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)).\n",
    "\n",
    "The building block of the Graph Attention Network is the graph attention layer, which is a variant of the aggregation function. Like before, a graph attention layer $l$ transforms the set of node features $\\mathbf{h^{l-1}} = \\{h_1^{l-1}, h_2^{l-1}, \\dots, h_N^{l-1}$\\}, $h_i^{l-1} \\in R^F$ $\\rightarrow$ $\\mathbf{h^{l}} = \\{h_1^{l}, h_2^{l}, \\dots, h_N^{l}$\\}, $h_i^{l} \\in R^{F'}$.\n",
    "\n",
    "Now let's see how this transformation is performed for each graph attention layer. First, a shared linear transformation parameterized by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ is applied to every node. \n",
    "\n",
    "Next, we perform self-attention on the nodes using a shared attention function $a$:\n",
    "\\begin{equation} \n",
    "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}\n",
    "\\end{equation}\n",
    "\n",
    "that computes the attention coefficients, capturing the importance of node $j$'s features to node $i$:\n",
    "\\begin{equation}\n",
    "e_{ij} = a(\\mathbf{W_{dst}}\\overrightarrow{h_i}, \\mathbf{W_{src}} \\overrightarrow{h_j})\n",
    "\\end{equation}\n",
    "\n",
    "The most general formulation of self-attention allows every node to attend to all other nodes, which drops all structural information. However, to utilize the graph structure in the attention mechanisms, we use **masked attention**. In masked attention, we only compute attention coefficients $e_{ij}$ for nodes $j \\in \\mathcal{N}_i$ where $\\mathcal{N}_i$ is the set of neighbors for node $i$ in the graph. Namely, we only compute attention along the edges of the graph. \n",
    "\n",
    "To easily compare coefficients across nodes, we normalize the coefficients across a node $i$'s neighbors $j$ using the softmax function:\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "\\end{equation}\n",
    "\n",
    "For this problem, our attention mechanism $a$ will be a single-layer feedforward neural network parametrized by weight vectors $\\overrightarrow{a_{src}} \\in \\mathbb{R}^{F'}$ and $\\overrightarrow{a_{dst}} \\in \\mathbb{R}^{F'}$, followed by a LeakyReLU nonlinearity (with negative input slope 0.2). Letting $\\cdot^T$ represent transposition, the coefficients computed by our attention mechanism may be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_{dst}}^T \\mathbf{W_{dst}} \\overrightarrow{h_i} + \\overrightarrow{a_{src}}^T\\mathbf{W_{src}}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_{dst}}^T \\mathbf{W_{dst}} \\overrightarrow{h_i} + \\overrightarrow{a_{src}}^T\\mathbf{W_{src}}\\overrightarrow{h_k}\\Big)\\Big)}\n",
    "\\end{equation}\n",
    "\n",
    "For the following questions, we denote `alpha_{src}` = $\\alpha_{src} = [...,\\overrightarrow{a_{src}}^T \\mathbf{W_{src}} \\overrightarrow{h_j},...] \\in \\mathcal{R}^n$ and `alpha_{dst}` = $\\alpha_{dst} = [..., \\overrightarrow{a_{dst}}^T \\mathbf{W_{dst}} \\overrightarrow{h_i}, ...] \\in \\mathcal{R}^n$, where again $n$ is the number of nodes in the graph.\n",
    "\n",
    "\n",
    "For every GAT layer, after the attention coefficients are computed, the aggregation function for a node $i$ is computed as a weighted sum over the messages from neighboring nodes $j$ **weighted by the attention weights** $\\alpha_{ij}$. These aggregated features will serve as the final output features for every node.\n",
    "\n",
    "\\begin{equation}\n",
    "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_{src}} \\overrightarrow{h_j}.\n",
    "\\end{equation}\n",
    "\n",
    "At this point, we have covered a lot of information! Before reading further about multi-head attention, we encourage you to go again through the exercise of thinking about what components of the attention mechanism correspond with the different functions: 1) `forward`, 2) `message`, and 3 `aggregate`. \n",
    "\n",
    "- Hint 1: in the `forward` method, the subsript `_src` refers to resources used on source/neighbor nodes, whereas subsript `_dst` refers to resources used on destination/target/central nodes\n",
    "- Hint 2: Our aggregation is very similar to that of GraphSage except now we are using sum aggregation.\n",
    "- Hint 3: The terms we aggregate over represent the individual message that each neighbor node j sends. Thus, we see that $\\alpha_{ij}$ is actually part of the message each node sends and should be computed during the message step. This makes sense since each attention weight is associated with a single edge in the graph.\n",
    "- Hint 4: Look at the terms in the definition of $\\alpha_{ij}$. What values can you pre-process and pass as parameters to the `propagate` function (i.e. those computed only over the nodes *that do not* require edge information). The parameters of `message(..., x_j, alpha_j, alpha_i, ...)` should give a good hint.  \n",
    "- Hint 5: Remember that parameters accessed in the `message` function are extracted from parameters passed to propagate using `edge_index`; thus, their first dimension has shape $|E|$.\n",
    "\n",
    "### Multi-Head Attention\n",
    "To stabilize the learning process of self-attention, you will use multi-head attention. To do this you use $K$ **independent** attention mechanisms, or ``heads'', to compute output features exactly as described in the above equations. Then, you simply concatenate these output feature representations:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_{src}}^{(k)} \\overrightarrow{h_j}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "where $||$ is concatenation, $\\alpha_{ij}^{(k)}$ are the normalized attention coefficients computed by the $k$-th attention mechanism $(a^k)$, and $\\mathbf{W_{src}}^{(k)}$ is the corresponding input linear transformation's weight matrix. Note that for this setting, $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$. \n",
    "\n",
    "**Note:** We recommend first thinking through the implementation of single head attention. Switching to multi-head attention then requires thinking through adding an extra dimension $K$ to effectively compute $K$ separate single head attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "w4j45gTpCeXO"
   },
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, args, **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = args.heads\n",
    "        self.negative_slope = args.negative_slope\n",
    "        self.dropout = args.dropout\n",
    "        bias = args.bias\n",
    "\n",
    "        self.lin_src = None\n",
    "        self.lin_dst = None\n",
    "        self.att_src = None\n",
    "        self.att_dst = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the layers needed for the message functions below.\n",
    "        # self.lin_src is the linear transformation that you apply to embeddings \n",
    "        # BEFORE message passing.\n",
    "        # \n",
    "        # Pay attention to dimensions of the linear layers, especially when\n",
    "        # implementing multi-head attention.\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.lin_src = nn.Linear(self.in_channels, self.heads * self.hidden_dim)\n",
    "        ############################################################################\n",
    "\n",
    "        self.lin_dst = self.lin_src\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the attention parameters \\overrightarrow{a_{src}/{dst}}^T in the above intro.\n",
    "        # 1. Be mindful of when you want to include multi-head attention.\n",
    "        # 2. Note that for each attention head we parametrize the attention parameters \n",
    "        #    as weight vectors NOT matrices - i.e. their first dimension should be 1.\n",
    "        # 3. Use nn.Parameter instead of nn.Linear\n",
    "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
    "        self.att_src = nn.Parameter(torch.randn(1,self.heads * self.hidden_dim))\n",
    "        self.att_dst = nn.Parameter(torch.randn(1,self.heads * self.hidden_dim))\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_src.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_dst.weight)\n",
    "        nn.init.xavier_uniform_(self.att_src)\n",
    "        nn.init.xavier_uniform_(self.att_dst)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
    "        # 1. First apply linear transformation to node embeddings, and split that \n",
    "        #    into multiple heads. We use the same representations for source and\n",
    "        #    target nodes, but apply different linear weights (W_{src} and W_{dst})\n",
    "        # 2. Calculate alpha vectors for central nodes (alpha_{dst}) and neighbor nodes (alpha_{src}).\n",
    "        # 3. Call propagate function to conduct the message passing. \n",
    "        #    3.1 Remember to pass alpha = (alpha_{src}, alpha_{dst}) as a parameter.\n",
    "        #    3.2 See here for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        # 4. Transform the output back to the shape of N * d.\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "        x_src = self.lin_src(x) # x is all nodes.  nodes are interpreted as src and dst\n",
    "        x_dst = self.lin_dst(x) # x is all nodes.  nodes are interpreted as src and dst\n",
    "        alpha_src = self.att_src * x_src\n",
    "        alpha_dst = self.att_dst * x_dst\n",
    "        #print(H, C, x.shape, x_src.shape, x_dst.shape, self.att_src.shape, self.att_dst.shape, alpha_src.shape, alpha_dst.shape)\n",
    "        out = self.propagate(edge_index, x=(x_src,x_dst), alpha=(alpha_src,alpha_dst))\n",
    "        #print('expected N*d: ',out.shape)\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your message function. Putting the attention in message \n",
    "        # instead of in update is a little tricky.\n",
    "        # 1. Calculate the attention weights using alpha_i and alpha_j,\n",
    "        #    and apply leaky ReLU.\n",
    "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
    "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
    "        # 3. Apply dropout to attention weights (alpha).\n",
    "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
    "        #    should be of shape (E, H, d).\n",
    "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
    "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "        att_wgts = alpha_i + alpha_j\n",
    "        att_wgts_norm = torch.nn.LeakyReLU(self.negative_slope)(att_wgts)\n",
    "        #print(x_j.shape, alpha_j.shape, alpha_i.shape, index.shape)\n",
    "        #print(att_wgts.shape, att_wgts_norm.shape)\n",
    "        soft = torch_geometric.utils.softmax(att_wgts_norm,index=index,ptr=ptr)\n",
    "        alpha = torch.nn.Dropout(p=self.dropout)(soft)\n",
    "        #print('x_j: ',x_j.shape,'alpha: ',alpha.shape)\n",
    "        out = (x_j * alpha).view(x_j.shape[0],self.heads,self.hidden_dim)\n",
    "        #print ('expected (E,H,d)',out.shape)\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
    "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "        out = torch_scatter.scatter(inputs, index, dim=0, dim_size=dim_size, reduce=\"sum\")\n",
    "        out = out.view(out.shape[0],out.shape[1]*out.shape[2])\n",
    "        ############################################################################\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2dkgSuWCheU"
   },
   "source": [
    "## Building Optimizers\n",
    "\n",
    "This function has been implemented for you. **For grading purposes please use the default Adam optimizer**, but feel free to play with other types of optimizers on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "f_TIQ8NPCjBP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBYdWFwYCkwY"
   },
   "source": [
    "## Training and Testing\n",
    "\n",
    "Here we provide you with the functions to train and test. **Please do not modify this part for grading purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "_tZMWRc8CmGg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "    \n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['test_mask'].numpy()))\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, args)\n",
    "    \n",
    "    # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
    "    # try:\n",
    "    #   model = torch_geometric.compile(model)\n",
    "    #   print(f\"GNNStack based on {args.model_type} Model compiled\")\n",
    "    # except Exception as err:\n",
    "    #   print(f\"Model compile not supported: {err}\")\n",
    "    \n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "    \n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "          print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('CORA-Node-' + model_type + '.csv', sep=',', index=False)\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def train_wrapper(args):\n",
    "\n",
    "    args = objectview(args)\n",
    "\n",
    "    if args.dataset == 'cora':\n",
    "        dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown dataset\") \n",
    "    test_accs, losses, best_model, best_acc, test_loader = train(dataset, args) \n",
    "\n",
    "    print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "    print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "    # Run test for our best model to save the predictions!\n",
    "    test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=args.model_type)\n",
    "    print()\n",
    "\n",
    "    out = {\n",
    "        \"model_type\": args.model_type,\n",
    "        \"dataset_name\": dataset.name,\n",
    "        \"losses\": losses, \n",
    "        \"test_accuracies\": test_accs\n",
    "    }\n",
    "\n",
    "    return objectview(out)\n",
    "  \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7-h7jIsCns4"
   },
   "source": [
    "## Let's Start the Training!\n",
    "\n",
    "You will be working on the CORA dataset on node-level classification.\n",
    "\n",
    "This part is implemented for you. **For grading purposes, please do not modify the default parameters.** However, feel free to play with different configurations just for fun!\n",
    "\n",
    "**Submit your best accuracy and loss on Gradescope.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRCr4iGfJLKT",
    "outputId": "cdcff784-2973-4188-996e-eaf4ae88cc50"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "        'model_type': 'GraphSage', \n",
    "        'dataset': 'cora', \n",
    "        'num_layers': 2, \n",
    "        'normalize': True, \n",
    "        'bias': False, \n",
    "        'heads': 1, \n",
    "        'negative_slope': 0.2, \n",
    "        'batch_size': 32, \n",
    "        'hidden_dim': 32, \n",
    "        'dropout': 0.5, \n",
    "        'epochs': 500, \n",
    "        'opt': 'adam', \n",
    "        'opt_scheduler': 'none', \n",
    "        'opt_restart': 0, \n",
    "        'weight_decay': 5e-3, \n",
    "        'lr': 0.01\n",
    "    }\n",
    "\n",
    "    graph_sage_plot_data = train_wrapper(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLPgtZ7cJLKU",
    "outputId": "c812bbe1-03e4-4534-aa0e-375a1b3fd09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node task. test set size: 1000\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_GRADESCOPE_ENV\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcora\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m     19\u001b[0m     }\n\u001b[1;32m---> 21\u001b[0m     gat_plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 113\u001b[0m, in \u001b[0;36mtrain_wrapper\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m--> 113\u001b[0m test_accs, losses, best_model, best_acc, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum test set accuracy: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mmax\u001b[39m(test_accs)))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinimum loss: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mmin\u001b[39m(losses)))\n",
      "Cell \u001b[1;32mIn[45], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, args)\u001b[0m\n\u001b[0;32m     24\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# build model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGNNStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_node_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#   model = torch_geometric.compile(model)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#   print(f\"GNNStack based on {args.model_type} Model compiled\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# except Exception as err:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#   print(f\"Model compile not supported: {err}\")\u001b[39;00m\n\u001b[0;32m     36\u001b[0m scheduler, opt \u001b[38;5;241m=\u001b[39m build_optimizer(args, model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mGNNStack.__init__\u001b[1;34m(self, input_dim, hidden_dim, output_dim, args, emb)\u001b[0m\n\u001b[0;32m     22\u001b[0m conv_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_conv_model(args\u001b[38;5;241m.\u001b[39mmodel_type)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mconv_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (args\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of layers is not >=1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[49], line 38\u001b[0m, in \u001b[0;36mGAT.__init__\u001b[1;34m(self, in_channels, out_channels, args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_src\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m############# Your code here #############\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Define the attention parameters \\overrightarrow{a_{src}/{dst}}^T in the above intro.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 1. Be mindful of when you want to include multi-head attention.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 3. Use nn.Parameter instead of nn.Linear\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Our implementation is ~2 lines, but don't worry if you deviate from this.\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_src \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_dst \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_dst)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\XCS224W\\lib\\site-packages\\torch\\nn\\parameter.py:49\u001b[0m, in \u001b[0;36mParameter.__new__\u001b[1;34m(cls, data, requires_grad)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_make_subclass(\u001b[38;5;28mcls\u001b[39m, data, requires_grad)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating a Parameter from an instance of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires that detach() returns an instance of the same type, but return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mits __torch_dispatch__() implementation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\XCS224W\\lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    args = {\n",
    "        'model_type': 'GAT', \n",
    "        'dataset': 'cora', \n",
    "        'num_layers': 2, \n",
    "        'normalize': True, \n",
    "        'bias': False, \n",
    "        'heads': 2, \n",
    "        'negative_slope': 0.2, \n",
    "        'batch_size': 32, \n",
    "        'hidden_dim': 32, \n",
    "        'dropout': 0.5, \n",
    "        'epochs': 500, \n",
    "        'opt': 'adam', \n",
    "        'opt_scheduler': 'none', \n",
    "        'opt_restart': 0, \n",
    "        'weight_decay': 5e-3, \n",
    "        'lr': 0.01\n",
    "    }\n",
    "\n",
    "    gat_plot_data = train_wrapper(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "zGMDAEZ8JLKV",
    "outputId": "a9a0004a-2e53-4a94-e7fe-ecbb8f508a8c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    \n",
    "    # GraphSage plot\n",
    "    if 'graph_sage_plot_data' in locals():\n",
    "        plt.title(graph_sage_plot_data.dataset_name)\n",
    "        plt.plot(graph_sage_plot_data.losses, label=\"training loss\" + \" - \" + graph_sage_plot_data.model_type)\n",
    "        plt.plot(graph_sage_plot_data.test_accuracies, label=\"test accuracy\" + \" - \" + graph_sage_plot_data.model_type)\n",
    "\n",
    "    # GAT plot\n",
    "    if 'gat_plot_data' in locals():\n",
    "        plt.title(gat_plot_data.dataset_name)\n",
    "        plt.plot(gat_plot_data.losses, label=\"training loss\" + \" - \" + gat_plot_data.model_type)\n",
    "        plt.plot(gat_plot_data.test_accuracies, label=\"test accuracy\" + \" - \" + gat_plot_data.model_type)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHELqjARZ1W5"
   },
   "source": [
    "## Question 1.1: What is the maximum accuracy obtained on the test set for GraphSage? (20 points)\n",
    "\n",
    "Running the cell above will show the results of your best model and save your best model's predictions to a file named *CORA-Node-GraphSage.csv*.  \n",
    "\n",
    "As you have seen before you can view this file by clicking on the *Folder* icon on the left side pannel. When you sumbit your assignment, you will have to download this file and attatch it to your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlCtBEBLMBkR"
   },
   "source": [
    "## Question 1.2: What is the maximum accuracy obtained on test set for GAT? (20 points)\n",
    "\n",
    "\n",
    "Running the training cell above will also save your best GAT model predictions as *CORA-Node-GAT.csv*.  \n",
    "\n",
    "When you sumbit your assignment, you will have to download this file and attatch it to your submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwwq0nSdmsOL"
   },
   "source": [
    "# 2) DeepSNAP Basics\n",
    "\n",
    "In previous Colabs, you have seen graph class (NetworkX) and tensor (PyG) representations of graphs. The graph class `nx.Graph` provides rich analysis and manipulation functionalities, such as computing the clustering coefficient and PageRank vector for a graph. When working with PyG you were then introduced to tensor based representation of graphs (i.e. edge tensor `edge_index` and node attributes tensors `x` and `y`). \n",
    "\n",
    "In this section, we present DeepSNAP, a package that combines the benefits of both graph representations and offers a full pipeline for GNN training / validation / and testing. Namely, DeepSNAP includes a graph class representation to allow for more efficient graph manipulation and analysis in addition to a tensor based representation for efficient message passing computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf7vUmdNKCjA"
   },
   "source": [
    "In general, [DeepSNAP](https://github.com/snap-stanford/deepsnap) is a Python library to assist efficient deep learning on graphs. DeepSNAP enables flexible graph manipulation, standard graph learning pipelines, heterogeneous graphs, and overall represents a simple graph learning API. In more detail:\n",
    "\n",
    "1. DeepSNAP allows for sophisticated graph manipulations, such as feature computation, pretraining, subgraph extraction etc. during/before training.\n",
    "2. DeepSNAP standardizes the pipelines for node, edge, and graph-level prediction tasks under inductive or transductive settings. Specifically, DeepSNAP removes previous non-trivial / repetative design choices left to the user, such as how to split datasets. DeepSNAP thus greatly saves in coding efforts and enables fair model comparison.\n",
    "3. Many real-world graphs are heterogeneous in nature (i.e. include different node types or edge types). However, most packages lack complete support for heterogeneous graphs, including data storage and flexible message passing. DeepSNAP provides an efficient and flexible heterogeneous graph that supports both node and edge heterogeneity.\n",
    "\n",
    "In this next section, you will use DeepSNAP for graph manipulation and dataset splitting.\n",
    "\n",
    "[DeepSNAP](https://github.com/snap-stanford/deepsnap) is a newly released project and it is still under development. If you find any bugs or have any improvement ideas, feel free to raise issues or create pull requests on the GitHub directly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20SvvngpQmmQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfbBVFmAQlwz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def visualize(G, color_map=None, seed=123):\n",
    "  if color_map is None:\n",
    "    color_map = '#c92506'\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  nodes = nx.draw_networkx_nodes(G, pos=nx.spring_layout(G, seed=seed), \\\n",
    "                                 label=None, node_color=color_map, node_shape='o', node_size=150)\n",
    "  edges = nx.draw_networkx_edges(G, pos=nx.spring_layout(G, seed=seed), alpha=0.5)\n",
    "  if color_map is not None:\n",
    "    plt.scatter([],[], c='#c92506', label='Nodes with label 0', edgecolors=\"black\", s=140)\n",
    "    plt.scatter([],[], c='#fcec00', label='Nodes with label 1', edgecolors=\"black\", s=140)\n",
    "    plt.legend(prop={'size': 13}, handletextpad=0)\n",
    "  nodes.set_edgecolor('black')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-o1P3r6hr2"
   },
   "source": [
    "## DeepSNAP Graph\n",
    "\n",
    "The `deepsnap.graph.Graph` class is the core class of DeepSNAP. It not only represents a graph in tensor format but also includes a graph object from a graph manipulation package.\n",
    "\n",
    "Currently DeepSNAP supports [NetworkX](https://networkx.org/) and [Snap.py](https://snap.stanford.edu/snappy/doc/index.html) as back end graph manipulation packages.\n",
    "\n",
    "In this Colab, you will focus on using NetworkX as the back end graph manipulation package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ispq_lIoJl_z"
   },
   "source": [
    "### NetworkX to DeepSNAP\n",
    "To begin, you will first work through converting a simple random NetworkX graph to a DeepSNAP graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "zT5qca3x6XpG",
    "outputId": "317347f7-a358-4c6a-e8ef-d90fcf052ccd"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_nodes = 100\n",
    "  p = 0.05\n",
    "  seed = 100\n",
    "\n",
    "  # Generate a networkx random graph\n",
    "  G = nx.gnp_random_graph(num_nodes, p, seed=seed)\n",
    "\n",
    "  # Generate some random node features and labels\n",
    "  node_feature = {node : torch.rand([5, ]) for node in G.nodes()}\n",
    "  node_label = {node : torch.randint(0, 2, ()) for node in G.nodes()}\n",
    "\n",
    "  # Set the random features and labels to G\n",
    "  nx.set_node_attributes(G, node_feature, name='node_feature')\n",
    "  nx.set_node_attributes(G, node_label, name='node_label')\n",
    "\n",
    "  # Print one node example\n",
    "  for node in G.nodes(data=True):\n",
    "    print(node)\n",
    "    break\n",
    "\n",
    "  color_map = ['#c92506' if node[1]['node_label'].item() == 0 else '#fcec00' for node in G.nodes(data=True)]\n",
    "\n",
    "  # Visualize the graph\n",
    "  visualize(G, color_map=color_map)\n",
    "\n",
    "  # Transform the networkx graph into the deepsnap graph\n",
    "  graph = Graph(G)\n",
    "\n",
    "  # Print out the general deepsnap graph information\n",
    "  print(graph)\n",
    "\n",
    "  # DeepSNAP will convert node attributes to tensors\n",
    "  # Notice the type of tensors\n",
    "  print(\"Node feature (node_feature) has shape {} and type {}\".format(graph.node_feature.shape, graph.node_feature.dtype))\n",
    "  print(\"Node label (node_label) has shape {} and type {}\".format(graph.node_label.shape, graph.node_label.dtype))\n",
    "\n",
    "  # DeepSNAP will also generate the edge_index tensor\n",
    "  print(\"Edge index (edge_index) has shape {} and type {}\".format(graph.edge_index.shape, graph.edge_index.dtype))\n",
    "\n",
    "  # Different from only storing tensors, deepsnap graph also references to the networkx graph\n",
    "  # We will discuss why the reference will be helpful later\n",
    "  print(\"The DeepSNAP graph has {} as the internal manupulation graph\".format(type(graph.G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNMbc307KOQD"
   },
   "source": [
    "### Tensor graph attributes\n",
    "\n",
    "Similar to the native PyG tensor based representation, DeepSNAP includes a graph tensor based representation with three levels of graph attributes. In this example, you primarily have **node level** attributes including `node_feature` and `node_label`. The other two levels of attributes are **edge** and **graph** attributes. Similar to node level attributes, these attributes are prefixed by their respective type. For example, the features become `edge_feature` or `graph_feature` and labels becomes `edge_label` or `graph_label` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8Xz58_Da0qL"
   },
   "source": [
    "### Graph Object\n",
    "DeepSNAP allows you to easily access graph information  through the backend graph object and graph manipulation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLo4zWAoeg6S",
    "outputId": "679a4b14-a3a5-4b56-af91-b7fe8b4689f0"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # Number of nodes\n",
    "  print(\"The random graph has {} nodes\".format(graph.num_nodes))\n",
    "\n",
    "  # Number of edges\n",
    "  print(\"The random graph has {} edges\".format(graph.num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po7IaRmwblI5"
   },
   "source": [
    "### PyG to DeepSNAP\n",
    "\n",
    "Lastly, DeepSNAP provides functionality to automatically transform a PyG dataset into a list of DeepSNAP graphs.\n",
    "\n",
    "Here you transform the CORA dataset into a list with one DeepSNAP graph (i.e. the singular CORA graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFkg2kCgcFwR",
    "outputId": "58e984f7-5b72-4781-a90b-7a93f66c49de"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cora'\n",
    "  name = 'Cora'\n",
    "\n",
    "  # The Cora dataset\n",
    "  pyg_dataset= Planetoid(root, name)\n",
    "\n",
    "  # PyG dataset to a list of deepsnap graphs\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Get the first deepsnap graph (CORA only has one graph)\n",
    "  graph = graphs[0]\n",
    "  print(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLm5vVYMAP2x"
   },
   "source": [
    "## Question 2.1: How many classes are in the CORA graph? How many features does each node have? (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iF_Kyqr_JbY",
    "outputId": "5628068a-5d2b-4634-f14b-13c0d2cad177"
   },
   "outputs": [],
   "source": [
    "def get_num_node_classes(graph):\n",
    "  # TODO: Implement a function that takes a deepsnap graph object\n",
    "  # and return the number of node classes of that graph.\n",
    "\n",
    "  num_node_classes = 0\n",
    "\n",
    "  ############# Your code here #############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful\n",
    "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
    "  pass\n",
    "  ##########################################\n",
    "\n",
    "  return num_node_classes\n",
    "\n",
    "def get_num_node_features(graph):\n",
    "  # TODO: Implement a function that takes a deepsnap graph object\n",
    "  # and return the number of node features of that graph.\n",
    "\n",
    "  num_node_features = 0\n",
    "\n",
    "  ############# Your code here #############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful\n",
    "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
    "  pass\n",
    "  ##########################################\n",
    "\n",
    "  return num_node_features\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_node_classes = get_num_node_classes(graph)\n",
    "  num_node_features = get_num_node_features(graph)\n",
    "  print(\"{} has {} classes\".format(name, num_node_classes))\n",
    "  print(\"{} has {} features\".format(name, num_node_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwKbzhHUAckZ"
   },
   "source": [
    "## DeepSNAP Dataset\n",
    "\n",
    "Now, you will learn how to create DeepSNAP datasets. A `deepsnap.dataset.GraphDataset` contains a list of `deepsnap.graph.Graph` objects. In addition to the list of graphs, you must specify what task the dataset will be used on: node level task (`task=node`), edge level task (`task=link_pred`) and graph level task (`task=graph`).\n",
    "\n",
    "The GraphDataset class contains many other useful parameters that can be specified during initialization. If you are interested, you can take a look at the [documentation](https://snap.stanford.edu/deepsnap/modules/dataset.html#deepsnap-graphdataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSidf9E0hn2s"
   },
   "source": [
    "As an example, let us first look at the COX2 dataset, which contains 467 graphs. In initializing our dataset, convert the PyG dataset into its corresponding DeepSNAP dataset and specify the task to `graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4kqUldyoaS_",
    "outputId": "265390e1-08c1-41c4-a88f-c4c56f459562"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:  \n",
    "  root = './tmp/cox2'\n",
    "  name = 'COX2'\n",
    "\n",
    "  # Load the dataset through PyG\n",
    "  pyg_dataset = TUDataset(root, name)\n",
    "\n",
    "  # Convert to a list of deepsnap graphs\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Convert list of deepsnap graphs to deepsnap dataset with specified task=graph\n",
    "  dataset = GraphDataset(graphs, task='graph')\n",
    "  print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sCV3xJWCddX"
   },
   "source": [
    "## Question 2.2: What is the label of the graph with index 100? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIis9oTZAfs3",
    "outputId": "9f25536c-5b60-4fdb-8c02-4701625212a2"
   },
   "outputs": [],
   "source": [
    "def get_graph_class(dataset, idx):\n",
    "  # TODO: Implement a function that takes a deepsnap dataset object,\n",
    "  # the index of a graph in the dataset, and returns the class/label \n",
    "  # of the graph (in integer).\n",
    "\n",
    "  label = -1\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  ## Notice\n",
    "  ## 1. The graph label refers to a graph-level attribute\n",
    "  pass\n",
    "  #########################################\n",
    "\n",
    "  return label\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  graph_0 = dataset[0]\n",
    "  print(graph_0)\n",
    "  idx = 100\n",
    "  label = get_graph_class(dataset, idx)\n",
    "  print('Graph with index {} has label {}'.format(idx, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKhcVeAhCwoY"
   },
   "source": [
    "## Question 2.3: How many edges are in the graph with index 200? (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5m2DOfhBtWv",
    "outputId": "a38191a1-5024-4c75-9b70-4ae507829e9e"
   },
   "outputs": [],
   "source": [
    "def get_graph_num_edges(dataset, idx):\n",
    "  # TODO: Implement a function that takes a deepsnap dataset object,\n",
    "  # the index of a graph in dataset, and returns the number of \n",
    "  # edges in the graph (in integer).\n",
    "\n",
    "  num_edges = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 lines of code)\n",
    "  ## Note\n",
    "  ## 1. You can use the class property directly\n",
    "  pass\n",
    "  #########################################\n",
    "\n",
    "  return num_edges\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  idx = 200\n",
    "  num_edges = get_graph_num_edges(dataset, idx)\n",
    "  print('Graph with index {} has {} edges'.format(idx, num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXa7yIG4E0Fp"
   },
   "source": [
    "# 3) DeepSNAP Advanced\n",
    "\n",
    "Now that you have learned the basics of DeepSNAP, let's move on to some more advanced functionalities.\n",
    "\n",
    "In this section you will use DeepSNAP for graph feature computation and transductive/inductive dataset splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5fsGBLY8cxa"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-jgRLiQ8cSj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnazPGGAJAZN"
   },
   "source": [
    "## Data Split in Graphs\n",
    "\n",
    "As discussed in the `Module 2: Introduction to Graph Neural Networks / 2.3  GNN Training Pipeline / Setting up GNN Prediction Tasks` Lecture, data splitting for graphs can be much harder than for CV or NLP.\n",
    "\n",
    "In general, data splitting is divided into two settings, **inductive** and **transductive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9KG_MhqsWBp"
   },
   "source": [
    "## Inductive Split\n",
    "\n",
    "In an inductive setting, you split a list of multiple graphs into disjoint training/valiation and test sets.\n",
    "\n",
    "Here is an example of using DeepSNAP to inductively split a list of graphs for a graph level task (graph classification etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gpc6bTm3GF02",
    "outputId": "846fa611-0369-4905-c519-a723a44fe40a"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cox2'\n",
    "  name = 'COX2'\n",
    "\n",
    "  pyg_dataset = TUDataset(root, name)\n",
    "\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Here we specify the task as graph-level task such as graph classification\n",
    "  task = 'graph'\n",
    "  dataset = GraphDataset(graphs, task=task)\n",
    "\n",
    "  # Specify transductive=False (inductive)\n",
    "  dataset_train, dataset_val, dataset_test = dataset.split(transductive=False, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "  print(\"COX2 train dataset: {}\".format(dataset_train))\n",
    "  print(\"COX2 validation dataset: {}\".format(dataset_val))\n",
    "  print(\"COX2 test dataset: {}\".format(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWKQwa4WsgQp"
   },
   "source": [
    "## Transductive Split\n",
    "\n",
    "In the transductive setting, the training /validation / test sets are all over the same graph. As discussed in the `Module 2: Introduction to Graph Neural Networks / 2.3  GNN Training Pipeline / Setting up GNN Prediction Tasks` Lecture, in a transductive setting, we do not need to generalize to new unseen graphs. \n",
    "\n",
    "As an example, here you transductively split the CORA graph for a node level task, such as node classification. \n",
    "\n",
    "(Notice that in DeepSNAP the default split setting is random (i.e. DeepSNAP randomly splits the e.g. nodes into train / val / test); however, you can also use a fixed split by specifying `fixed_split=True` when loading the dataset from PyG or changing the `node_label_index` directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5OdxSg4sfyR",
    "outputId": "23cd84ef-8703-4871-da37-6ca19e5f9ae4"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cora'\n",
    "  name = 'Cora'\n",
    "\n",
    "  pyg_dataset = Planetoid(root, name)\n",
    "\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Here we specify the task as node-level task such as node classification\n",
    "  task = 'node'\n",
    "\n",
    "  dataset = GraphDataset(graphs, task=task)\n",
    "\n",
    "  # Specify we want the transductive splitting\n",
    "  dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "  print(\"Cora train dataset: {}\".format(dataset_train))\n",
    "  print(\"Cora validation dataset: {}\".format(dataset_val))\n",
    "  print(\"Cora test dataset: {}\".format(dataset_test))\n",
    "\n",
    "  print(\"Original Cora has {} nodes\".format(dataset.num_nodes[0]))\n",
    "\n",
    "  # The nodes in each set can be find in node_label_index\n",
    "  print(\"After the split, Cora has {} training nodes\".format(dataset_train[0].node_label_index.shape[0]))\n",
    "  print(\"After the split, Cora has {} validation nodes\".format(dataset_val[0].node_label_index.shape[0]))\n",
    "  print(\"After the split, Cora has {} test nodes\".format(dataset_test[0].node_label_index.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7ePKgM00lGE"
   },
   "source": [
    "## Edge Level Split\n",
    "\n",
    "Compared to node and graph level splitting, edge level splitting is a little bit tricky ;)\n",
    "\n",
    "For edge level splitting you need to consider several different tasks:\n",
    "\n",
    "1. Splitting positive edges into train / val / test datasets.\n",
    "2. Sampling / re-sampling negative edges (i.e. edges not present in the graph).\n",
    "3. Splitting edges into message passing and supervision edges.\n",
    "\n",
    "With regard to point 3, for edge level data splitting we classify edges into two types. The first is `message passing` edges, edges that are used for message passing by our GNN. The second is `supervision`, edges that are used in the loss function for backpropagation. DeepSNAP allows for two different modes, where the `message passing` and `supervision` edges are either the same or disjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnzISX5RoiR6"
   },
   "source": [
    "### All Edge Splitting Mode\n",
    "\n",
    "First, you will explore the `edge_train_mode=\"all\"` mode for edge level splitting, where the `message passing` and `supervision` edges are shared during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_D104xO6137n",
    "outputId": "b9647236-2787-4306-d987-16a15bf196a1"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cora'\n",
    "  name = 'Cora'\n",
    "\n",
    "  pyg_dataset = Planetoid(root, name)\n",
    "\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  # Specify task as link_pred for edge-level task\n",
    "  task = 'link_pred'\n",
    "\n",
    "  # Specify the train mode, \"all\" mode is default for deepsnap dataset\n",
    "  edge_train_mode = \"all\"\n",
    "\n",
    "  dataset = GraphDataset(graphs, task=task, edge_train_mode=edge_train_mode)\n",
    "\n",
    "  # Transductive link prediction split\n",
    "  dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "  print(\"Cora train dataset: {}\".format(dataset_train))\n",
    "  print(\"Cora validation dataset: {}\".format(dataset_val))\n",
    "  print(\"Cora test dataset: {}\".format(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GscopwOXC_Y7"
   },
   "source": [
    "In DeepSNAP, the indices of supervision edges are stored in the `edge_label_index` tensor and the corresponding edge labels are stored in `edge_label` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJF8fZnA2eLR",
    "outputId": "9a478f9a-e4e9-4811-84ba-c7d473738c16"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  print(\"Original Cora graph has {} edges\".format(dataset[0].num_edges))\n",
    "  print()\n",
    "\n",
    "  print(\"Train set has {} message passing edge\".format(dataset_train[0].edge_index.shape[1] // 2))\n",
    "  print(\"Train set has {} supervision (positive) edges\".format(dataset_train[0].edge_label_index.shape[1] // 4))\n",
    "\n",
    "  print()\n",
    "  print(\"Validation set has {} message passing edge\".format(dataset_val[0].edge_index.shape[1] // 2))\n",
    "  print(\"Validation set has {} supervision (positive) edges\".format(dataset_val[0].edge_label_index.shape[1] // 4))\n",
    "\n",
    "  print()\n",
    "  print(\"Test set has {} message passing edge\".format(dataset_test[0].edge_index.shape[1] // 2))\n",
    "  print(\"Test set has {} supervision (positive) edges\".format(dataset_test[0].edge_label_index.shape[1] // 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6BX-I_oEKQX"
   },
   "source": [
    "**Specific things to note in `all` mode**:\n",
    "\n",
    "* At training time: the supervision edges are the same as the training message passing edges.  \n",
    "* At validation time: the message passing edges are the training message passing edges and training supervision edges (still the training message passing edges in this case). However, you now include a set of unseen validation supervision edges that are disjoint from the training supervision edges.\n",
    "* At test time: the message passing edges are the union of training message passing edges, training supervision edges, and validation supervision edges. The test supervision edges then disjoint from the training supervision edges and validation supervision edges.\n",
    "* For this illustration negative edges are excluded. However, the attributes `edge_label` and `edge_label_index` naturally also include the negative supervision edges (by default the number of negative edges is the same as the number of positive edges, hence the divide by 4 above).\n",
    "\n",
    "\n",
    "Now, that you have seen the basics of the `all` method for edge splitting, you will implement a function that checks whether two edge index tensors are disjoint and explore more edge splitting properties by using that function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOZHDskbAKN6"
   },
   "source": [
    "## Question 3: Implement a function that checks whether two edge_index tensors are disjoint (i.e. do not share any common edges). Then answer the True/False questions below. (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgRYdyPp8EmO"
   },
   "outputs": [],
   "source": [
    "def edge_indices_disjoint(edge_index_1, edge_index_2):\n",
    "  # TODO: Implement this function that takes two edge index tensors,\n",
    "  # and returns whether these two edge index tensors are disjoint.\n",
    "  disjoint = None\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~5 lines of code)\n",
    "  ## Note\n",
    "  ## 1. Here disjoint means that there is no single edge belonging to both edge index tensors\n",
    "  ## 2. You do not need to consider the undirected case. For example, if edge_index_1 contains\n",
    "  ## edge (a, b) and edge_index_2 contains edge (b, a). We will treat them as disjoint in this\n",
    "  ## function.\n",
    "  pass\n",
    "  #########################################\n",
    "\n",
    "  return disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EL4ASIDDEIUf",
    "outputId": "b8373719-4d4d-4be0-9562-c76a2a0da439"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_train_edges = dataset_train[0].edge_label_index.shape[1] // 2\n",
    "  train_pos_edge_index = dataset_train[0].edge_label_index[:, :num_train_edges]\n",
    "  train_neg_edge_index = dataset_train[0].edge_label_index[:, num_train_edges:]\n",
    "  print(\"3.1 Training (supervision) positive and negative edges are disjoint = {}\"\\\n",
    "          .format(edge_indices_disjoint(train_pos_edge_index, train_neg_edge_index)))\n",
    "\n",
    "  num_val_edges = dataset_val[0].edge_label_index.shape[1] // 2\n",
    "  val_pos_edge_index = dataset_val[0].edge_label_index[:, :num_val_edges]\n",
    "  val_neg_edge_index = dataset_val[0].edge_label_index[:, num_val_edges:]\n",
    "  print(\"3.2 Validation (supervision) positive and negative edges are disjoint = {}\"\\\n",
    "          .format(edge_indices_disjoint(val_pos_edge_index, val_neg_edge_index)))\n",
    "\n",
    "  num_test_edges = dataset_test[0].edge_label_index.shape[1] // 2\n",
    "  test_pos_edge_index = dataset_test[0].edge_label_index[:, :num_test_edges]\n",
    "  test_neg_edge_index = dataset_test[0].edge_label_index[:, num_test_edges:]\n",
    "  print(\"3.3 Test (supervision) positive and negative edges are disjoint = {}\"\\\n",
    "          .format(edge_indices_disjoint(test_pos_edge_index, test_neg_edge_index)))\n",
    "\n",
    "  print(\"3.4 Test (supervision) positive and validation (supervision) positive edges are disjoint = {}\"\\\n",
    "          .format(edge_indices_disjoint(test_pos_edge_index, val_pos_edge_index)))\n",
    "  print(\"3.5 Validation (supervision) positive and training (supervision) positive edges are disjoint = {}\"\\\n",
    "          .format(edge_indices_disjoint(val_pos_edge_index, train_pos_edge_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jLoVN5ZBTuA"
   },
   "source": [
    "### Disjoint Edge Splitting Mode\n",
    "\n",
    "Now you will look at a relatively more complex transductive edge split setting, the `edge_train_mode=\"disjoint\"` mode in DeepSNAP. In this setting, the `message passing` and `supervision` edges are completely disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Rqzfb-0BTBm",
    "outputId": "27efeb22-50d1-4abf-a706-d23ab0749d9b"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:  \n",
    "  edge_train_mode = \"disjoint\"\n",
    "\n",
    "  dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=edge_train_mode)\n",
    "  orig_edge_index = dataset[0].edge_index\n",
    "  dataset_train, dataset_val, dataset_test = dataset.split(\n",
    "      transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "\n",
    "  train_message_edge_index = dataset_train[0].edge_index\n",
    "  train_sup_edge_index = dataset_train[0].edge_label_index\n",
    "  val_message_edge_index = dataset_val[0].edge_index\n",
    "  val_sup_edge_index = dataset_val[0].edge_label_index\n",
    "  test_message_edge_index = dataset_test[0].edge_index\n",
    "  test_sup_edge_index = dataset_test[0].edge_label_index\n",
    "\n",
    "  print(\"Original Cora graph has {} edges\".format(dataset[0].num_edges))\n",
    "  print()\n",
    "  print(\"Train set has {} message passing edge\".format(train_message_edge_index.shape[1] // 2))\n",
    "  print(\"Train set has {} supervision (positive) edges\".format(train_sup_edge_index.shape[1] // 4))\n",
    "\n",
    "  print()\n",
    "  print(\"Validation set has {} message passing edge\".format(val_message_edge_index.shape[1] // 2))\n",
    "  print(\"Validation set has {} supervision (positive) edges\".format(val_sup_edge_index.shape[1] // 4))\n",
    "\n",
    "  print()\n",
    "  print(\"Test set has {} message passing edge\".format(test_message_edge_index.shape[1] // 2))\n",
    "  print(\"Test set has {} supervision (positive) edges\".format(test_sup_edge_index.shape[1] // 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUkBhiJNciol"
   },
   "source": [
    "\n",
    "**Specific things to note in `disjoint` mode**:\n",
    "\n",
    "* At training time: the training supervision edges are disjoint from the training message passing edges.\n",
    "* At validation time: the message passing edges are the union of training message passing edges and training supervision edges. The validation supervision edges are disjoint from both the training message passing and supervision edges.\n",
    "* At test time: the message passing edges are the training message passing edges, training supervision edges, and validation supervision edges. The test supervision edges are disjoint from all the training and validation edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WKfRjqAJHtK"
   },
   "source": [
    "## Negative Edges\n",
    "\n",
    "For edge level tasks, sampling negative edges (edges not present in the graph) is critical. Moreover, during each training iteration, you want to resample the negative edges.\n",
    "\n",
    "Below we print the training and validation sets negative edges in two training iterations.\n",
    "\n",
    "What we demonstrate is that the negative edges are only resampled during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMEbnx63JHWj",
    "outputId": "91371ed4-6a65-4487-ca68-8a5cd7bde7ab"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:  \n",
    "  dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=\"disjoint\")\n",
    "  datasets = {}\n",
    "  follow_batch = []\n",
    "  datasets['train'], datasets['val'], datasets['test'] = dataset.split(\n",
    "      transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
    "  dataloaders = {\n",
    "    split: DataLoader(\n",
    "      ds, collate_fn=Batch.collate(follow_batch),\n",
    "      batch_size=1, shuffle=(split=='train')\n",
    "    )\n",
    "    for split, ds in datasets.items()\n",
    "  }\n",
    "  neg_edges_1 = None\n",
    "  for batch in dataloaders['train']:\n",
    "    num_edges = batch.edge_label_index.shape[1] // 2\n",
    "    neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
    "    print(\"First iteration training negative edges:\")\n",
    "    print(neg_edges_1)\n",
    "    break\n",
    "  neg_edges_2 = None\n",
    "  for batch in dataloaders['train']:\n",
    "    num_edges = batch.edge_label_index.shape[1] // 2\n",
    "    neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
    "    print(\"Second iteration training negative edges:\")\n",
    "    print(neg_edges_2)\n",
    "    break\n",
    "\n",
    "  neg_edges_1 = None\n",
    "  for batch in dataloaders['val']:\n",
    "    num_edges = batch.edge_label_index.shape[1] // 2\n",
    "    neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
    "    print(\"First iteration validation negative edges:\")\n",
    "    print(neg_edges_1)\n",
    "    break\n",
    "  neg_edges_2 = None\n",
    "  for batch in dataloaders['val']:\n",
    "    num_edges = batch.edge_label_index.shape[1] // 2\n",
    "    neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
    "    print(\"Second iteration validation negative edges:\")\n",
    "    print(neg_edges_2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEzqh7wEdrh0"
   },
   "source": [
    "If you are interested in more graph splitting settings, please refer to the DeepSNAP dataset [documentation](https://snap.stanford.edu/deepsnap/modules/dataset.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkrYyeSUI_9_"
   },
   "source": [
    "## Graph Transformation and Feature Computation\n",
    "\n",
    "The other core functionality of DeepSNAP is graph transformation / feature computation.\n",
    "\n",
    "In DeepSNAP, graph transformation / feature computation is divided into two different types. The first includes transformations before training (e.g. transform the whole dataset before training directly), and the second includes transformations during training (transform batches of graphs).\n",
    "\n",
    "Below is an example that uses the NetworkX back end to calculate the PageRank value for each node and subsequently transforms the node features by concatenating each nodes PageRank score (transform the dataset before training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnAVbZINLZ4I",
    "outputId": "de1d97f7-5ddc-4f02-9022-972d01ef6cf7"
   },
   "outputs": [],
   "source": [
    "def pagerank_transform_fn(graph):\n",
    "\n",
    "  # Get the referenced networkx graph\n",
    "  G = graph.G\n",
    "\n",
    "  # Calculate the pagerank by using networkx\n",
    "  pr = nx.pagerank(G)\n",
    "\n",
    "  # Transform the pagerank values to tensor\n",
    "  pr_feature = torch.tensor([pr[node] for node in range(graph.num_nodes)], dtype=torch.float32)\n",
    "  pr_feature = pr_feature.view(graph.num_nodes, 1)\n",
    "\n",
    "  # Concat the pagerank values to the node feature\n",
    "  graph.node_feature = torch.cat([graph.node_feature, pr_feature], dim=-1)\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './tmp/cox2'\n",
    "  name = 'COX2'\n",
    "  pyg_dataset = TUDataset(root, name)\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "  dataset = GraphDataset(graphs, task='graph')\n",
    "  print(\"Number of features before transformation: {}\".format(dataset.num_node_features))\n",
    "  dataset.apply_transform(pagerank_transform_fn, update_tensor=False)\n",
    "  print(\"Number of features after transformation: {}\".format(dataset.num_node_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHByE87SQkUw"
   },
   "source": [
    "## Question 4: Implement a transformation that adds the clustering coefficient of each node to its feature vector and then report the clustering coefficient of the node with index 3 in the graph with index 406 (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNEjfOZRNjYb",
    "outputId": "ffe17379-ac6c-4ae7-b3dd-88c4db2856e8"
   },
   "outputs": [],
   "source": [
    "def cluster_transform_fn(graph):\n",
    "  # TODO: Implement a function that takes an deepsnap graph object and \n",
    "  # transform the graph by adding each node's clustering coefficient to its \n",
    "  # graph.node_feature representation\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~5 lines of code)\n",
    "  ## Note\n",
    "  ## 1. Compute the clustering coefficient value for each node and\n",
    "  ## concat this value to the last dimension of graph.node_feature\n",
    "  pass\n",
    "  #########################################\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './cox2'\n",
    "  name = 'COX2'\n",
    "  pyg_dataset = TUDataset(root, name)\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "  dataset = GraphDataset(graphs, task='graph')\n",
    "\n",
    "  # Transform the dataset\n",
    "  dataset.apply_transform(cluster_transform_fn, update_tensor=False)\n",
    "\n",
    "  node_idx = 3\n",
    "  graph_idx = 406\n",
    "  node_feature = dataset[graph_idx].node_feature\n",
    "\n",
    "  print(\"The node has clustering coefficient: {}\".format(round(node_feature[node_idx][-1].item(), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P5Ig7XaPYzp"
   },
   "source": [
    "### Final Thoughts\n",
    "Apart from transforming the whole dataset before training, DeepSNAP can also transform the graph (usually sampled batches of graphs, `deepsnap.batch.Batch`) during each training iteration.\n",
    "\n",
    "Also, DeepSNAP supports the synchronization of the transformation between the referenced graph objects and tensor representations. For example, you can just update the NetworkX graph object in the transform function, and by specifying `update_tensor=True` the internal tensor representations will be automatically updated!\n",
    "\n",
    "For more information, please refer to the DeepSNAP [documentation](https://snap.stanford.edu/deepsnap/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-YLYMLFQYqp"
   },
   "source": [
    "# 4) Edge Level Prediction\n",
    "\n",
    "From the last section, you learned how DeepSNAP transductively splits edges for edge level tasks. For the last part of the notebook, you will use DeepSNAP and PyG together to implement a simple edge level prediction (link prediction) model!\n",
    "\n",
    "Specifically, you will use a 2 layer GraphSAGE embedding model to generate node embeddings, and then compute link predictions through a dot product link prediction head. Namely, given an edge (u, v) with GNN feature embeddings $f_u$ and $f_v$, our link prediction head generates its link prediction as $f_u \\cdot f_v$. \n",
    "\n",
    "To give a brief intuition for this dot product link prediction model, we are learning a GNN that embedds nodes such that nodes that have an edge in the graph are closer within the embedding space than nodes that do not have an edge. The dot product provides a proxy for closeness in our embedding space where a high positive dot product indicates that two vectors are more closely aligned (the angle between the vectors is small), whereas a negative dot-product indicates that vectors are unaligned (the angle between the vectors is greater than 90). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrKCNtvERypQ"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset\n",
    "from torch_geometric.datasets import Planetoid, TUDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class LinkPredModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.2):\n",
    "        super(LinkPredModel, self).__init__()\n",
    "\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.loss_fn = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        ## (~1 line of code)\n",
    "        ## Note\n",
    "        ## 1. Initialize the loss function to BCEWithLogitsLoss\n",
    "        pass\n",
    "        ##########################################\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        node_feature, edge_index, edge_label_index = batch.node_feature, batch.edge_index, batch.edge_label_index\n",
    "        \n",
    "        ############# Your code here #############\n",
    "        ## (~6 line of code)\n",
    "        ## Note\n",
    "        ## 1. Feed the node feature into the first conv layer\n",
    "        ## 2. Add a ReLU after the first conv layer\n",
    "        ## 3. Add dropout after the ReLU (with probability self.dropout)\n",
    "        ## 4. Feed the output to the second conv layer\n",
    "        ## 5. Select the embeddings of the source and destination \n",
    "        ## nodes of each edge by using the edge_label_index and \n",
    "        ## compute the dot product similarity of each pair.\n",
    "        pass\n",
    "        ##########################################\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, pred, link_label):\n",
    "        return self.loss_fn(pred, link_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuKbGFOu1Ka8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def train(model, dataloaders, optimizer, args):\n",
    "    val_max = 0\n",
    "    best_model = model\n",
    "\n",
    "    for epoch in range(1, args[\"epochs\"]):\n",
    "        for i, batch in enumerate(dataloaders['train']):\n",
    "            \n",
    "            batch.to(args[\"device\"])\n",
    "            model.train()\n",
    "\n",
    "            ############# Your code here #############\n",
    "            ## (~5 lines of code)\n",
    "            ## Note\n",
    "            ## 1. Zero grad the optimizer\n",
    "            ## 2. Get the model predictions for the current batch\n",
    "            ## 3. Convert the true labels to the same datatype as the predictions\n",
    "            ## 4. Compute loss and backpropagate\n",
    "            ## 5. Update the model parameters\n",
    "            pass\n",
    "            ##########################################\n",
    "\n",
    "            log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Loss: {}'\n",
    "            score_train = test(model, dataloaders['train'], args)\n",
    "            score_val = test(model, dataloaders['val'], args)\n",
    "            score_test = test(model, dataloaders['test'], args)\n",
    "\n",
    "            print(log.format(epoch, score_train, score_val, score_test, loss.item()))\n",
    "            if val_max < score_val:\n",
    "                val_max = score_val\n",
    "                best_model = copy.deepcopy(model)\n",
    "    return best_model\n",
    "\n",
    "def test(model, dataloader, args, save_model_preds=False):\n",
    "    model.eval()\n",
    "\n",
    "    score = 0\n",
    "    preds = None\n",
    "    labels = None\n",
    "\n",
    "    ############# Your code here #############\n",
    "    ## (~7 lines of code)\n",
    "    ## Note\n",
    "    ## 1. Loop through batches in the dataloader (Note for us there is only one batch!)\n",
    "    ## 2. Feed the batch to the model\n",
    "    ## 3. Feed the model output to sigmoid\n",
    "    ## 4. Compute the ROC-AUC score by using sklearn roc_auc_score function\n",
    "    ##    Note: Look into flattening and converting torch tensors into numpy arrays\n",
    "    ## 5. Edge labels are stored in batch.edge_label\n",
    "    ## 6. Make sure to save your **numpy** model predictions as 'preds' \n",
    "    ##    and the **numpy** edge labels as 'labels'\n",
    "    pass\n",
    "    ##########################################\n",
    "\n",
    "    if save_model_preds:\n",
    "        print (\"Saving Link Classification Model Predictions\")\n",
    "        print()\n",
    "\n",
    "        data = {}\n",
    "        data['pred'] = preds\n",
    "        data['label'] = labels\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save locally as csv\n",
    "        df.to_csv('CORA-Link-Prediction.csv', sep=',', index=False)\n",
    " \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTKWYX1b33V3"
   },
   "outputs": [],
   "source": [
    "# Please don't change any parameters\n",
    "args = {\n",
    "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"hidden_dim\" : 128,\n",
    "    \"epochs\" : 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Klw_xYnE27xQ",
    "outputId": "13d40392-36f5-4cf0-e738-696fc7089c34"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  pyg_dataset = Planetoid('./tmp/cora', 'Cora')\n",
    "  graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
    "\n",
    "  dataset = GraphDataset(\n",
    "          graphs,\n",
    "          task='link_pred',\n",
    "          edge_train_mode=\"disjoint\"\n",
    "      )\n",
    "  datasets = {}\n",
    "  datasets['train'], datasets['val'], datasets['test']= dataset.split(\n",
    "              transductive=True, split_ratio=[0.85, 0.05, 0.1])\n",
    "  input_dim = datasets['train'].num_node_features\n",
    "  num_classes = datasets['train'].num_edge_labels\n",
    "\n",
    "  model = LinkPredModel(input_dim, args[\"hidden_dim\"], num_classes).to(args[\"device\"])\n",
    "\n",
    "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
    "  # try:\n",
    "  #   model = torch_geometric.compile(model)\n",
    "  #   print(f\"LinkPredModel Model compiled\")\n",
    "  # except Exception as err:\n",
    "  #   print(f\"Model compile not supported: {err}\")\n",
    "\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "  dataloaders = {split: DataLoader(\n",
    "              ds, collate_fn=Batch.collate([]),\n",
    "              batch_size=1, shuffle=(split=='train'))\n",
    "              for split, ds in datasets.items()}\n",
    "  best_model = train(model, dataloaders, optimizer, args)\n",
    "  log = \"Best Model Accuraies Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\"\n",
    "  best_train_roc = test(best_model, dataloaders['train'], args)\n",
    "  best_val_roc = test(best_model, dataloaders['val'], args)\n",
    "  best_test_roc = test(best_model, dataloaders['test'], args, save_model_preds=True)\n",
    "  print(log.format(best_train_roc, best_val_roc, best_test_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5brlsKElP0_"
   },
   "source": [
    "## Question 5: What is the maximum ROC-AUC score you get for your best_model on test set? (15 points)\n",
    "\n",
    "\n",
    "After training your model, download and submit your best model prediction file: *CORA-Link-Prediction.csv*.  \n",
    "\n",
    "As we have seen before you can view this file by clicking on the *Folder* icon on the left side pannel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JXsMTBgeOI"
   },
   "source": [
    "# Submission\n",
    "\n",
    "You will need to submit four files on Gradescope to complete this notebook. \n",
    "\n",
    "1.   Your completed *XCS224W_Colab3.ipynb*. From the \"File\" menu select \"Download .ipynb\" to save a local copy of your completed Colab. \n",
    "2.  *CORA-Node-GraphSage.csv* \n",
    "3.  *CORA-Node-GAT.csv*\n",
    "4.  *CORA-Link-Prediction.csv*\n",
    "\n",
    "Download the csv files by selecting the *Folder* icon on the left panel. \n",
    "\n",
    "To submit your work, zip the files downloaded in steps 1-4 above and submit to gradescope. **NOTE:** DO NOT rename any of the downloaded files. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f924b1a48e95c8c2f8ccec074c3308df864242dc668404ff7f1c5b20503f9f26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
